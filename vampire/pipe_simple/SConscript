import json
import os
from os.path import join
import re

import numpy as np  # noqa

import nestly
import nestly.scons as ns
import SCons.Script as sc

import common
import tcr_vae

from common import cluster_execution_string

sc.Import('env')
localenv = env.Clone()  # noqa


def check_mode():
    if localenv['mode'] not in ['mini', 'default']:
        raise Exception(f"Unknown mode '{localenv['mode']}'")


def apply_mode(l):
    """
    Default mode runs everything, and mini mode just runs a single element from the list.
    """
    check_mode()
    if localenv['mode'] == 'mini':
        return [l[0]]
    else:
        return l


def default_params_by_mode():
    """
    Mini mode doesn't train for long.
    """
    params = tcr_vae.TCRVAE.default_params()

    if localenv['mode'] == 'mini':
        params['pretrains'] = 2
        params['warmup_period'] = 3
        params['epochs'] = 10

    return params


def base_dict():
    """
    The dictionary that will be shared by all the nests.

    nseqs: the number of sequences generated by the various programs, and for
    which we evaluate Pvae for on the real data.

    train_size: the number of sequences to take for training.

    max_q: the level for truncating q_{lvj} in the thymic Q calculation.
    """

    d = {'nseqs': 10000,
         'train_size': 100000,
         'max_q': 100,
         }

    if localenv['mode'] == 'mini':
        d['nseqs'] = 100

    # Slurp up all key-value pairs in the JSON file into the base_dict.
    with open(localenv['data']) as fp:
        for k, v in json.load(fp).items():
            d[k] = v

    return d


def numerical_nest_add(nest_name, number_list):
    """
    Add an nest for a list of non-negative numbers, with nice zero-padded
    directory names.
    """
    nest.add(nest_name, apply_mode(number_list), label_func=common.zero_pad_list_func(number_list))


# ### Nests and targets ###

nest = ns.SConsWrap(nestly.Nest(base_dict=base_dict()), alias_environment=localenv)

# Nest: the data set choice, named via data name prepended with `_output_`.
nest.add('data_label', [localenv['data']], label_func=lambda p: '_output_' + common.strip_dirpath_extn(p))

# The test_set_agg allows us to process the test sets once in the things that don't depend on the VAE.
nest.add_aggregate('test_set_agg', list)
# The test_set_info_agg gathers information about the processed test sets so we can get at them later.
nest.add_aggregate('test_set_info_agg', dict)
# The summarized_agg gathers everything we want to summarize and then stack at the end.
nest.add_aggregate('summarized_agg', list)
# The merged gathers merged per-sequence information on the test set.
nest.add_aggregate('merged_agg', list)
summarized_agg_names = []
# nest.add_aggregate('loss_regression_agg', list)


@nest.add_target_with_env(localenv)
def sconscript(env, outdir, c):
    """
    Copy SConscript file into output directory.
    """
    return env.Command(
        join(outdir, 'SConscript'),
        'SConscript',
        'cp $SOURCE $TARGET')[0]


@nest.add_target_with_env(localenv)
def json_data(env, outdir, c):
    """
    Copy JSON data file into output directory.
    """
    return env.Command(
        join(outdir, os.path.basename(localenv['data'])),
        localenv['data'],
        'cp $SOURCE $TARGET')[0]


@nest.add_target_with_env(localenv)
def vampire_version(env, outdir, c):
    """
    Echo vampire version to file.
    """
    return env.Command(
        join(outdir, 'vampire-version.txt'),
        [],
        'python -c "import vampire; print(vampire.__version__)" > $TARGET')[0]


# Nest: initial processing of the test sets.
nest.add('test_data', lambda c: c['test_paths'], label_func=common.strip_dirpath_extn)


@nest.add_target_with_env(localenv)
def test_head(env, outdir, c):
    """
    Run the preprocess_adaptive.py script on the test data, which incorporates
    downsampling.
    """
    test_head_path = join(outdir, common.strip_dirpath_extn(c['test_data'])+'.head.csv')
    # Remove a pesky and meaningless leading `./` that was making trouble later.
    c['test_set_agg'].append(re.sub('^\./', '', test_head_path))
    return env.Command(
        test_head_path,
        c['test_data'],
        f"python3 preprocess_adaptive.py --sample {c['nseqs']} $SOURCE $TARGET")[0]


@nest.add_target_with_env(localenv)
def test_pgen(env, outdir, c):
    test_pgen_path = join(outdir, 'test-head.pgen.tsv')
    c['test_set_info_agg'][(str(c['test_head']), 'test_pgen')] = test_pgen_path
    return env.Command(
        test_pgen_path,
        c['test_head'],
        cluster_execution_string('adaptive-pgen.sh {sources} {targets}', localenv, 0))[0]


nest.pop('test_data')


# Nest: the training data.
# By using a nest we retain the option of having multiple training sets later.
nest.add('train_data', lambda c: [c['train_tsv_path']], label_func=common.strip_dirpath_extn)
summarized_agg_names.append('train_data')


# Nest: training set size
# numerical_nest_add('train_size', np.floor(common.logspace(5000, 500000, 6)))
# summarized_agg_names.append('train_size')


@nest.add_target_with_env(localenv)
def preprocess(env, outdir, c):
    """
    Run the preprocess_adaptive.py script on the training/validation data.
    """
    in_path = c['train_data']
    # We take enough for training and validation_head, with nseqs to spare.
    n_to_sample = c['train_size'] + 2 * c['nseqs']
    return env.Command(
        join(outdir, 'train.processed.csv'),
        in_path,
        f'python3 preprocess_adaptive.py --sample {n_to_sample} $SOURCE $TARGET')[0]


@nest.add_target_with_env(localenv, 'split')
@ns.name_targets
def split(env, outdir, c):
    """
    Split the training data into training and validation.
    """
    in_path = c['preprocess']
    return 'training', 'validation', env.Command(
        [join(outdir, 'training.csv'), join(outdir, 'validation.csv')],
        in_path,
        f"python3 util.py split --train-size {c['train_size']} $SOURCE $TARGETS")

@nest.add_target_with_env(localenv)
def training_head(env, outdir, c):
    return env.Command(
        common.strip_extn(c['split']['training'])+'.head.csv',
        c['split']['training'],
        f"head -n {c['nseqs']+1} $SOURCE > $TARGET")[0]


@nest.add_target_with_env(localenv)
def training_head_pgen(env, outdir, c):
    """
    Get Pgen for some of the training data.
    """
    return env.Command(
        join(outdir, 'training.head.pgen.tsv'),
        c['training_head'],
        'adaptive-pgen.sh $SOURCE $TARGET')[0]


@nest.add_target_with_env(localenv)
def validation_head(env, outdir, c):
    return env.Command(
        common.strip_extn(c['split']['validation'])+'.head.csv',
        c['split']['validation'],
        f"head -n {c['nseqs']+1} $SOURCE > $TARGET")[0]


@nest.add_target_with_env(localenv)
def validation_pgen(env, outdir, c):
    return env.Command(
        join(outdir, common.strip_dirpath_extn(c['validation_head'])+'.pgen.csv'),
        c['validation_head'],
        cluster_execution_string('adaptive-pgen.sh {sources} {targets}', localenv, 0))[0]

# Nest: the dimension of the latent space.
# numerical_nest_add('warmup_period', [5*i for i in range(0,6)])
# summarized_agg_names.append('warmup_period')

# Nest: the dimension of the latent space.
# numerical_nest_add('latent_dim', [30, 35, 40])
# summarized_agg_names.append('latent_dim')

# Nest: the number of dense nodes.
# numerical_nest_add('dense_nodes', [50, 75, 100, 125, 150])
# summarized_agg_names.append('dense_nodes')

# Nest: the amino acid embedding dimension.
# numerical_nest_add('aa_embedding_dim', [10, 15, 20, 21])
# summarized_agg_names.append('aa_embedding_dim')

# Nest: the V gene embedding dimension.
# numerical_nest_add('v_gene_embedding_dim', [20, 30, 40])
# summarized_agg_names.append('v_gene_embedding_dim')

# Nest: the strength of the KL component of the VAE loss.
# numerical_nest_add('beta', np.linspace(0.625, 1, 7))
numerical_nest_add('beta', [0.75])
summarized_agg_names.append('beta')

# Nest: monitor for EarlyStopping.
# nest.add('stopping_monitor', ['loss', 'val_loss'])
# summarized_agg_names.append('stopping_monitor')

# Nest: EarlyStopping patience.
# numerical_nest_add('patience', [5*x for x in range(7)])
# summarized_agg_names.append('patience')


# Nest: the model.
# This needs to appear in the innermost nest so that we can pop it and then do some OLGA work.
# In order to keep things happy when merging with OLGA results, don't comment
# this out if you are only interested in one model. Instead, make a
# single-model nest.
# nest.add('model', apply_mode([common.strip_dirpath_extn(m) for m in glob.glob('../models/*.py')]))
nest.add('model', ['basic', 'count_match'])
summarized_agg_names.append('model')


@nest.add_target_with_env(localenv)
def model_params(env, outdir, c):
    """
    Write out a file with parameters from which we can build our VAE.
    """

    # Copy over any relevant parameters from c into the params dictionary.
    params = default_params_by_mode()
    for k, v in c.items():
        if k in params:
            params[k] = v

    return env.Command(
        join(outdir, 'model_params.json'),
        [],
        f"echo '{json.dumps(params)}' > $TARGET")[0]


@nest.add_target_with_env(localenv, 'trained')
@ns.name_targets
def trained(env, outdir, c):
    return 'weights', 'diagnostics', env.Command(
        [join(outdir, 'best_weights.h5'), join(outdir, 'diagnostics.csv')],
        [c['model_params'], c['split']['training']],
        cluster_execution_string('tcr-vae train {sources} {targets}', localenv))


# @nest.add_target_with_env(localenv)
# def loss(env, outdir, c):
#     loss = join(outdir, 'loss.csv')
#     return env.Command(
#         loss,
#         [c['model_params'], c['trained']['weights'], c['split']['training'],
#             c['split']['validation']],
#         cluster_execution_string('tcr-vae loss {sources} {targets}'))[0]


@nest.add_target_with_env(localenv)
def training_pvae(env, outdir, c):
    pvae_call = f"tcr-vae pvae --limit-input-to {c['nseqs']}"
    return env.Command(
        join(outdir, common.strip_dirpath_extn(c['split']['training'])+'.head.pvae.csv'),
        [c['model_params'], c['trained']['weights'], c['split']['training']],
        cluster_execution_string(pvae_call + ' {sources} {targets}', localenv))[0]


@nest.add_target_with_env(localenv)
def validation_pvae(env, outdir, c):
    return env.Command(
        join(outdir, common.strip_dirpath_extn(c['validation_head'])+'.pvae.csv'),
        [c['model_params'], c['trained']['weights'], c['validation_head']],
        cluster_execution_string('tcr-vae pvae {sources} {targets}', localenv))[0]


@nest.add_target_with_env(localenv)
def vae_generated(env, outdir, c):
    return env.Command(
        join(outdir, 'vae-generated.csv'),
        [c['model_params'], c['trained']['weights']],
        f"tcr-vae generate --nseqs {c['nseqs']} $SOURCES $TARGET")[0]


@nest.add_target_with_env(localenv)
def vae_generated_pgen(env, outdir, c):
    return env.Command(
        join(outdir, 'vae-generated.pgen.tsv'),
        c['vae_generated'],
        cluster_execution_string('adaptive-pgen.sh {sources} {targets}', localenv, 0))[0]

@nest.add_target_with_env(localenv)
def vae_generated_pvae(env, outdir, c):
    return env.Command(
        join(outdir, 'vae-generated.pvae.csv'),
        [c['model_params'], c['trained']['weights'], c['vae_generated']],
        cluster_execution_string('tcr-vae pvae {sources} {targets}', localenv))[0]

